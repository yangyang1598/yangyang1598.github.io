{"pages":[],"posts":[{"title":"The first day I made hexo","text":"github 연동 hexo 생성 1일차 좋은 포트폴리오 제작을 위해 꾸준히 업로드 할 수 있도록 하자.","link":"/2021/10/19/hello-world/"},{"title":"team프로젝트 코드","text":"team project main code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459import myopencv as myimport cv2, copyimport numpy as npdef binary(thresold, binary_img): binary = np.zeros_like(binary_img) binary[(binary_img &gt;= thresold[0]) &amp; (binary_img &lt;= thresold[1])] = 255 return binary passdef babo(img,src,dst): height, width = img.shape[:2] dst_size = (width, height) src = src * np.float32([width, height]) ## width, height 비율 값 dst = dst * np.float32(dst_size) ## 이미지를 적용할 화면 비율 M = cv2.getPerspectiveTransform(src, dst) ## 자를 이미지 좌표값 img_src = cv2.warpPerspective(img, M, dst_size) ## 잘라낼 이미지, 잘라낼 이미지 영역값, 잘라낼 이미지를 붙일 영역 사이즈 return img_src passdef Yolo(img, score, nms): height, width = img.shape[:2] yolo_net = cv2.dnn.readNet('YOLO/yolov3.weights', 'YOLO/yolov3.cfg') layer_names = yolo_net.getLayerNames() output_layers = [layer_names[i[0] - 1] for i in yolo_net.getUnconnectedOutLayers()] blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False) yolo_net.setInput(blob) outs = yolo_net.forward(output_layers) class_ids = [] confidences = [] boxes = [] for out in outs: for detection in out: scores = detection[5:] class_id = np.argmax(scores) confidence = scores[class_id] # 검출 신뢰도 if confidence &gt; 0.5: # Object detected # 검출기의 경계상자 좌표는 0 ~ 1로 정규화되어있으므로 다시 전처리 center_x = int(detection[0] * width) center_y = int(detection[1] * height) dw = int(detection[2] * width) dh = int(detection[3] * height) # Rectangle coordinate x = int(center_x - dw / 2) y = int(center_y - dh / 2) boxes.append([x, y, dw, dh]) confidences.append(float(confidence)) class_ids.append(class_id) indexes = cv2.dnn.NMSBoxes(boxes, confidences, score, nms) for i in range(len(boxes)): if i in indexes: x, y, w, h = boxes[i] label = str(classes[class_ids[i]]) score = confidences[i] text = f'{label} {score:.2f}' # 경계상자와 클래스 정보 투영 cv2.rectangle(img, (x, y), (x + w, y + h), (50, 50, 50), 2) cv2.rectangle(img, (x, y - 19), (int(x + len(text) * 2 * 4.5), y - 18 + len(text) * 2), (50, 50, 50), -1) cv2.putText(img, text, (x, y - 5), cv2.FONT_ITALIC, 0.5, (255, 255, 255), 2) return img passclasses = [&quot;person&quot;, &quot;bicycle&quot;, &quot;car&quot;, &quot;motorcycle&quot;, &quot;airplane&quot;, &quot;bus&quot;, &quot;train&quot;, &quot;truck&quot;, &quot;boat&quot;, &quot;traffic light&quot;, &quot;fire hydrant&quot;, &quot;stop sign&quot;, &quot;parking meter&quot;, &quot;bench&quot;, &quot;bird&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;horse&quot;, &quot;sheep&quot;, &quot;cow&quot;, &quot;elephant&quot;, &quot;bear&quot;, &quot;zebra&quot;, &quot;giraffe&quot;, &quot;backpack&quot;, &quot;umbrella&quot;, &quot;handbag&quot;, &quot;tie&quot;, &quot;suitcase&quot;, &quot;frisbee&quot;, &quot;skis&quot;, &quot;snowboard&quot;, &quot;sports ball&quot;, &quot;kite&quot;, &quot;baseball bat&quot;, &quot;baseball glove&quot;, &quot;skateboard&quot;, &quot;surfboard&quot;, &quot;tennis racket&quot;, &quot;bottle&quot;, &quot;wine glass&quot;, &quot;cup&quot;, &quot;fork&quot;, &quot;knife&quot;, &quot;spoon&quot;, &quot;bowl&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;sandwich&quot;, &quot;orange&quot;, &quot;broccoli&quot;, &quot;carrot&quot;, &quot;hot dog&quot;, &quot;pizza&quot;, &quot;donut&quot;, &quot;cake&quot;, &quot;chair&quot;, &quot;couch&quot;, &quot;potted plant&quot;, &quot;bed&quot;, &quot;dining table&quot;, &quot;toilet&quot;, &quot;tv&quot;, &quot;laptop&quot;, &quot;mouse&quot;, &quot;remote&quot;, &quot;keyboard&quot;, &quot;cell phone&quot;, &quot;microwave&quot;, &quot;oven&quot;, &quot;toaster&quot;, &quot;sink&quot;, &quot;refrigerator&quot;, &quot;book&quot;, &quot;clock&quot;, &quot;vase&quot;, &quot;scissors&quot;, &quot;teddy bear&quot;, &quot;hair drier&quot;, &quot;toothbrush&quot;]margin = 150nwindows = 9minpix = 1img_ = []count = 0frame_start = 0frame_count = 0frame = 0A = 0step = 1 # 한번에 넘어갈 프레임수line_len = 100road_width = 2.5_ = Trueleft_a, left_b, left_c = [], [], []right_a, right_b, right_c = [], [], []leftx_base_ = []rightx_base_ = []leftx_base_step = []rightx_base_step = []leftx_ = []lefty_ = []rightx_ = []righty_ = []left_fit_ = np.empty(3)right_fit_ = np.empty(3)name = ['REC_2021_10_09_10_16_45_F', 'REC_2021_10_09_10_17_45_F', 'REC_2021_10_09_10_18_45_F']Video = cv2.VideoCapture(f'video/1/{name[count]}.mp4')Out = cv2.VideoWriter('show_orginal_1.mp4', cv2.VideoWriter_fourcc(*'FMP4'), 30, (1920, 1080 + 270))frame_end = int(Video.get(cv2.CAP_PROP_FRAME_COUNT))passwhile True: frame_count += step _, img = Video.read() img_blur = cv2.blur(img, (5, 5), 0) img_hsv = cv2.cvtColor(img_blur, cv2.COLOR_BGR2HSV) img = my.Undistort(img, 'wide_dist_pickle.p') height, width = img.shape[:2] img_cut = np.zeros_like(img) if A == 0: A = 1 pass ## 블랙박스 TopLeft = (.4, .65) TopRight = (.55, .65) BottomLeft = (.0, .95) BottomRight = (.9, .95) ## 블랙박스 ## 흰색 white_lower = (15, 5, 90) white_upper = (255, 255, 255) ## 흰색 factor = np.float32([width, height]) src = np.float32([TopLeft, TopRight, BottomLeft, BottomRight]) dst = np.float32([(.0, .0), (1., .0), (.0, 1.), (1., 1.)]) cut = np.float32([TopLeft, TopRight, BottomRight, BottomLeft]) cut = np.int_(cut * factor) cv2.fillPoly(img_cut, [cut], (255, 255, 255)) img_src = cv2.bitwise_and(img_hsv, img_cut) img_temp = babo(img_src, src, dst) img_perspect = babo(img, src, dst) temp = np.zeros_like(img) temp_1 = copy.deepcopy(img_perspect) temp_2 = copy.deepcopy(img_perspect) temp_3 = copy.deepcopy(img_perspect) temp_4 = copy.deepcopy(img_perspect) temp_5 = copy.deepcopy(img_perspect) Yolo(img, 0.1, 0.4) img_white = cv2.inRange(img_temp, white_lower, white_upper) img_white_line = cv2.bitwise_and(img_perspect, img_perspect, mask=img_white) img_white_line_blur = cv2.blur(img_white_line, (3, 3), 1) img_hls = cv2.cvtColor(img_white_line_blur, cv2.COLOR_BGR2HLS) img_hls_h, img_hls_l, img_hls_s = cv2.split(img_hls) s_thresold = (120, 255) h_thresold = (150, 255) img_binary_1 = binary(h_thresold, img_hls_h) img_binary_2 = binary(s_thresold, img_hls_l) img_binary = cv2.addWeighted(img_binary_1, 1., img_binary_2, 1., 0) ## 네모 상자 그리기 histogram = np.sum(img_binary[height // 2:, :], axis=0) ## histogram 설명 : numpy.sum 설명 읽기 ## x,y 2차원 배열을 y축 반틈 아래 부분을 y축을 제거 하고 ## x축의 값만 더한 값 midpoint = int(histogram.shape[0] / 2) ## midpoint 설명 : histogram의 반틈 길이 좌,우 나누기 위함 leftx_base = np.argmax(histogram[:midpoint]) ## leftx_base 설명 : histogram의 0 ~ midpoint 중 제일 큰값 rightx_base = np.argmax(histogram[midpoint:]) + midpoint ## rightx_base 설명 : histogram의 midpoint ~ end 중 제일 큰값 leftx_base_.append(leftx_base) rightx_base_.append(rightx_base) leftx_base = np.int64(np.mean(leftx_base_[-10:])) rightx_base = np.int64(np.mean(rightx_base_[-10:])) window_height = int(height / nwindows) ## window_height 설명 : 사각형 범위 높이 겟수 nonzero = img_binary.nonzero() ## nonzero 설명 : 0이 아닌 값인 x, y 인덱스값 분리 nonzero_y = np.array(nonzero[0]) ## nonzero_y 설명 : y축의 0이 아닌 인덱스 nonzero_x = np.array(nonzero[1]) ## nonzero_x 설명 : x축의 0이 아닌 인덱스 leftx_current = leftx_base rightx_current = rightx_base left_lane_inds = [] right_lane_inds = [] # 네모 상자 그리기 for window in range(nwindows): win_y_low = height - (window + 1) * window_height win_y_high = height - window * window_height win_xleft_low = leftx_current - margin win_xleft_high = leftx_current + margin win_xright_low = rightx_current - margin win_xright_high = rightx_current + margin cv2.rectangle(temp, (win_xleft_low, win_y_low), (win_xleft_high, win_y_high), (100, 255, 255), 3) cv2.rectangle(temp, (win_xright_low, win_y_low), (win_xright_high, win_y_high), (100, 255, 255), 3) cv2.rectangle(temp_1, (win_xleft_low, win_y_low), (win_xleft_high, win_y_high), (100, 255, 255), 3) cv2.rectangle(temp_1, (win_xright_low, win_y_low), (win_xright_high, win_y_high), (100, 255, 255), 3) good_left_inds = ((nonzero_y &gt;= win_y_low) &amp; (nonzero_y &lt; win_y_high) &amp; (nonzero_x &gt;= win_xleft_low) &amp; (nonzero_x &lt; win_xleft_high)).nonzero()[0] ## good_left_inds 설명 : 왼쪽 사각형 범위 안에 x값의 0이 아닌 값 추출 good_right_inds = ((nonzero_y &gt;= win_y_low) &amp; (nonzero_y &lt; win_y_high) &amp; (nonzero_x &gt;= win_xright_low) &amp; (nonzero_x &lt; win_xright_high)).nonzero()[0] ## good_right_inds 설명 : 오른쪽 사각형 범위 안에 x값의 0이 아닌 값 추출 left_lane_inds.append(good_left_inds) ## left_lane_inds 설명 : good_left_inds 값을 left_lane_inds에 저장 right_lane_inds.append(good_right_inds) ## right_lane_inds 설명 : good_right_inds 값을 right_lane_inds 저장 leftx_base_step.append(leftx_current) rightx_base_step.append(rightx_current) if len(good_left_inds) &gt; minpix: leftx_current = int(np.mean(nonzero_x[good_left_inds])) ## leftx_current 설명 : nonzero_x 안에 good_left_inds 인덱스 값의 평균 값 pass elif len(good_left_inds) == 0: leftx_current = int(np.mean(leftx_base_step[-10:])) pass if len(good_right_inds) &gt; minpix: rightx_current = int(np.mean(nonzero_x[good_right_inds])) ## rightx_current 설명 : nonzero_x 안에 good_right_inds 인덱스 값의 평균 값 pass elif len(good_right_inds) == 0: rightx_current = int(np.mean(rightx_base_step[-10:])) pass # 네모 상자 그리기 # 네모 상자 그리기 left_lane_inds = np.concatenate(left_lane_inds) right_lane_inds = np.concatenate(right_lane_inds) ## left_lane_inds, right_lane_inds 설명 : 2차 배열을 1차 배열로 합침 leftx = nonzero_x[left_lane_inds] ## leftx 설명 : nonzero_x 안에 left_lane_inds 인덱스 값 lefty = nonzero_y[left_lane_inds] ## lefty 설명 : nonzero_y 안에 left_lane_inds 인덱스 값 rightx = nonzero_x[right_lane_inds] ## rightx 설명 : nonzero_x 안에 right_lane_inds 인덱스 값 righty = nonzero_y[right_lane_inds] ## righty 설명 : nonzero_y 안에 right_lane_inds 인덱스 값 leftx_.append(leftx) lefty_.append(lefty) rightx_.append(rightx) righty_.append(righty) if len(left_lane_inds) == 0: for i in range(len(leftx_)): if len(leftx_[-(i + 1)]) != 0: leftx = leftx_[-(i + 1)] lefty = lefty_[-(i + 1)] print(&quot;왼쪽 에러&quot;) break pass pass if len(right_lane_inds) == 0: for i in range(len(rightx_)): if len(rightx_[-(i + 1)]) != 0: rightx = rightx_[-(i + 1)] righty = righty_[-(i + 1)] print(&quot;오른쪽 에러&quot;) break pass pass pass left_fit = np.polyfit(lefty, leftx, 2) ## left_fit 설명 : np.polyfit를 통해 lefty, leftx 값에 대한 차수가 2인 값을 반환 (곡선) right_fit = np.polyfit(righty, rightx, 2) ## right_fit 설명 : np.polyfit를 통해 righty, rightx 값에 대한 차수가 2인 값을 반환 (곡선) left_a.append(left_fit[0]) left_b.append(left_fit[1]) left_c.append(left_fit[2]) right_a.append(right_fit[0]) right_b.append(right_fit[1]) right_c.append(right_fit[2]) left_fit_[0] = np.mean(left_a[-10:]) left_fit_[1] = np.mean(left_b[-10:]) left_fit_[2] = np.mean(left_c[-10:]) right_fit_[0] = np.mean(right_a[-10:]) right_fit_[1] = np.mean(right_b[-10:]) right_fit_[2] = np.mean(right_c[-10:]) ## 설명 : 아래와 같다 # left_fit_[0] = left_fit[0] # left_fit_[1] = left_fit[1] # left_fit_[2] = left_fit[2] # right_fit_[0] = right_fit[0] # right_fit_[1] = right_fit[1] # right_fit_[2] = right_fit[2] ploty = np.linspace(0, height - 1, height) ## ploty 설명 : np.linspace 통해 0 ~ 이미지 height 값 까지 값을 순서대로 배열 생성 left_fitx = left_fit_[0] * ploty ** 2 + left_fit_[1] * ploty + left_fit_[2] ## left_fitx 설명 : X = a[0] * y^2 + a[1] * y + a[2] // 2차 다항식 회귀 공식 right_fitx = right_fit_[0] * ploty ** 2 + right_fit_[1] * ploty + right_fit_[2] ## right_fitx 설명 : X = a[0] * y^2 + a[1] * y + a[2] // 2차 다항식 회귀 공식 mid_fitx = (left_fitx + right_fitx) // 2 temp[nonzero_y[left_lane_inds], nonzero_x[left_lane_inds]] = [255, 0, 100] temp[nonzero_y[right_lane_inds], nonzero_x[right_lane_inds]] = [0, 100, 255] temp_2[nonzero_y[left_lane_inds], nonzero_x[left_lane_inds]] = [255, 0, 100] temp_2[nonzero_y[right_lane_inds], nonzero_x[right_lane_inds]] = [0, 100, 255] left = np.array([np.transpose(np.vstack([left_fitx, ploty]))]) right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))]) mid = np.array([np.transpose(np.vstack([mid_fitx, ploty]))]) points = np.hstack((left, right)) left_center = np.int_(np.mean(left_fitx)) right_center = np.int_(np.mean(right_fitx)) mid_center = np.int_(np.mean(mid_fitx)) road_center = width // 2 road_pixel = road_width / (right_center - left_center) error = mid_center - road_center error_pixel = error * road_pixel cv2.line(temp_3, (left_center + 10, height // 2 + line_len), (left_center - 10, height // 2 - line_len), (255, 0, 255), 30) cv2.line(temp_3, (right_center + 10, height // 2 + line_len), (right_center - 10, height // 2 - line_len), (255, 0, 255), 30) cv2.line(temp_3, (mid_center + 10, height // 2 + line_len), (mid_center - 10, height // 2 - line_len), (255, 0, 255), 30) cv2.line(temp_3, (width // 2 - 10, height // 2), (width // 2 + 20, height), (255, 0, 255), 10) cv2.line(temp_3, (mid_center, height // 2), (width // 2 - 10, height // 2), (255, 255, 255), 30) cv2.polylines(temp, np.int_(points), False, (0, 255, 255), 10) cv2.polylines(temp_3, np.int_(points), False, (0, 255, 255), 10) cv2.polylines(temp_3, np.int_(mid), False, (0, 255, 255), 10) cv2.fillPoly(temp, np.int_(points), (0, 255, 0)) cv2.fillPoly(temp_4, np.int_(points), (0, 255, 0)) cv2.line(temp, (left_center + 10, height // 2 + line_len), (left_center - 10, height // 2 - line_len), (255, 0, 255), 30) cv2.line(temp, (right_center + 10, height // 2 + line_len), (right_center - 10, height // 2 - line_len), (255, 0, 255), 30) cv2.line(temp, (mid_center + 10, height // 2 + line_len), (mid_center - 10, height // 2 - line_len), (255, 0, 255), 30) cv2.line(temp, (width // 2 - 10, height // 2), (width // 2 + 20, height), (255, 0, 255), 10) cv2.line(temp, (mid_center, height // 2), (width // 2 - 10, height // 2), (255, 255, 255), 30) temp = babo(temp, dst, src) img_result = cv2.addWeighted(img, 1., temp, 0.4, 0) if error &gt; 0: cv2.putText(img_result, f'right : {abs(error_pixel):.2f}m', (width // 2 - 50, height // 2 + 200), cv2.FONT_ITALIC, 0.5, (0, 0, 255), 2) elif error &lt; 0: cv2.putText(img_result, f'left : {abs(error_pixel):.2f}m', (width // 2 - 50, height // 2 + 200), cv2.FONT_ITALIC, 0.5, (0, 0, 255), 2) elif error == 0: cv2.putText(img_result, f'center', (width // 2, height // 2), cv2.FONT_ITALIC, 0.5, (0, 0, 255), 2) cv2.putText(img_result, f'{frame} / {frame_end}', (20, height - 40), cv2.FONT_ITALIC, 0.5, (255, 255, 255), 2) cv2.putText(img_result, f'{frame_count} / {frame_end * 3}', (20, height - 20), cv2.FONT_ITALIC, 0.5, (255, 255, 255), 2) a = cv2.hconcat([temp_1, temp_2, temp_3, temp_4]) _, top = my.imgsize(a, 1920) result = cv2.vconcat([top, img_result]) _, result = my.imgsize(result, 1920) my.imgshow(&quot;result&quot;, result, 1000) Out.write(result) key = cv2.waitKey(1) frame += step Video.set(cv2.CAP_PROP_POS_FRAMES, frame) if Video.get(cv2.CAP_PROP_POS_FRAMES) ==\\ Video.get(cv2.CAP_PROP_FRAME_COUNT): if count &lt; 2: count += 1 frame = 0 # frame_count = 0 Video = cv2.VideoCapture(f'video/1/{name[count]}.mp4') print(name[count]) Video.set(cv2.CAP_PROP_POS_FRAMES, 0) frame_count = frame_end * count frame_end = int(Video.get(cv2.CAP_PROP_FRAME_COUNT)) print(f&quot;{count}&quot;) elif count &gt; 2: print(f&quot;저장 완료&quot;) break pass if key == 115: ## s cv2.waitKey(0) pass elif key == 102: ## f frame_start = int(input(&quot;&quot;)) frame_count += frame_start frame = frame_start Video.set(cv2.CAP_PROP_POS_FRAMES, frame_start) cv2.waitKey(0) elif key == 114: ## r Video.set(cv2.CAP_PROP_POS_FRAMES, frame_end - 1) frame = frame_end - 1 elif key == 27: Video.release() Out.release() cv2.destroyAllWindows() break pass passVideo.release()Out.release()cv2.destroyAllWindows()pass team project myopencv 파일12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import cv2, pickleimport numpy as npdef Url(link): n = np.fromfile(link, dtype=np.uint8) return ndef standard(src, size): ## src = 불러온 이미지 소스 , size = 변경할 크기 max = 0 if size &lt; src.shape[0] or size &lt; src.shape[1]: if src.shape[0] &gt; src.shape[1]: max = src.shape[0] elif src.shape[1] &gt; src.shape[0]: max = src.shape[1] else: max = src.shape[0] else: # print(&quot;확대 불가&quot;) max = size # print(&quot;원래이미지 h, w&quot;,src.shape[0],src.shape[1]) scale = size / max return scaledef imgsize(src, size): ## src = 불러온 이미지 소스 , size = 변경할 크기 scale = standard(src, size) return scale,cv2.resize(src, None, fx=scale, fy=scale)def imgcolor(src,Colortype=cv2.IMREAD_COLOR): ## src = 불러온 이미지 소스 , Colortype = 변경할 이미지 컬러(미지정시 자동 BGR컬로) return cv2.cvtColor(src,Colortype)def Imgread(link, size, Colortype=cv2.IMREAD_COLOR): ## url = 이미지 링크 , size = 변경할 크기 , 받아올 이미지 컬러타입 , Colortype = 변경할 이미지 컬러(미지정시 자동 BGR컬로) url = Url(link) if size &gt; 0: src = cv2.imdecode(url, Colortype) scale, result = imgsize(src, size) print(&quot;이미지 재지정 h, w&quot;, result.shape[0], result.shape[1]) return scale,result elif size == 0: src = cv2.imdecode(url, Colortype) return srcdef Undistort(img, url='camera_cal/wide_dist_pickle.p'): ## 보정 파라미터 사용 with open(url, mode='rb') as f: file = pickle.load(f) mtx = file['mtx'] dist = file['dist'] return cv2.undistort(img, mtx, dist, None, mtx)def binary(thresold, binary_img): binary = np.zeros_like(binary_img) binary[(binary_img &gt;= thresold[0]) &amp; (binary_img &lt;= thresold[1])] = 255 return binary passdef imgshow(title, src , size = 500): _, img = imgsize(src,size) return cv2.imshow(title, img) passdef imgmerge(img): img_src = cv2.merge([img, img, img]) return img_src passif __name__ ==&quot; __main__&quot;: pass","link":"/2021/10/19/team%20code/"},{"title":"개발자로써 익혀야 할 것들","text":"리눅스 명령어 모음git bash를 다루기 위해서는 리눅스 명령어 More : LINUX 명령어 모음 머신러닝 직군 질문모음개발자 및 머신러닝 직군의 면접 예상 질문 More : 개발자 면접질문 강사님 캐글강사님 캐글의 예시 글 More:강사님 TPS-April 파이토치 입문파이토치 : 2016년에 발표된 딥러닝을 구현을 위한 파이썬 기반의 오픈소스 머신러닝 라이브러리TensorFlow와 같이 오픈소스로써 제공됨. More:파이토치 입문","link":"/2021/10/19/tip/"},{"title":"파이토치 실습 및 코드","text":"GPU 환경으로 변경12345import torchif torch.cuda.is_available(): device = torch.device(&quot;cuda:0&quot;) 데이터 셋 구성 1234from torchvision import datasetsPATH_DATA = &quot;./data&quot;train_data = datasets.MNIST(PATH_DATA, train = True, download=True) -입력 데이터 및 대상 label 추출 12345678910X_train, y_train = train_data.data, train_data.targetsprint(X_train.shape)print(y_train.shape)val_data = datasets.MNIST(PATH_DATA, train = False, download=True)print(val_data) #X_val, y_val = val_data.data, val_data.targetsprint(X_val.shape)print(y_val.shape) 12345678910if len(X_train.shape) == 3: # 1인 차원을 생성(10,2,7-&gt;10,1,2,7) # squeeze의 경우 1인 차원을 삭제함(3,1,2-&gt;3,2) X_train = X_train.unsqueeze(1) print(X_train.shape)if len(X_val.shape) == 3: X_val = X_val.unsqueeze(1) print(X_val.shape) 데이터 시각화 진행 12345678910111213141516171819from torchvision import utilsimport matplotlib.pyplot as pltimport numpy as np%matplotlib inlinedef show(img): # convert tensor to numpy array np_img = img.numpy() # Convert to H * W * C shape np_img_tr = np.transpose(np_img, (1, 2, 0)) plt.imshow(np_img_tr, interpolation='nearest')X_grid = utils.make_grid(X_train[:20], nrow=4, padding=2) #batch가 포함된 dataloader 이미지 텐서들을 도중에 보고자 할 때 사용print(X_grid.shape)show(X_grid)","link":"/2021/10/21/pytorch_training/"},{"title":"","text":"MLP 모델이란? 퍼셉트론이 지니고 있는 한계(비선형 분류 문제 해결)를 극복하기 위해 여러 Layer를 쌓아올린 MLP(Multi Layer Perceptron)가 등장함. 간단히 비교하면, 퍼셉트론은 Input Layer와 Output Layer만 존재하는 형태입니다. MLP는 Input과 Output 사이에 Hidden Layer를 추가합니다. 즉, MLP는 이러한 Hidden Layer를 여러겹으로 쌓게 되는데, 이를 MLP 모델이라고 부릅니다. MLP 모델을 활용한 MNIST 모델 개발 MLP 모델 순서대로 코드를 구현합니다. Step 1. 모듈 불러오기123456import numpy as npimport matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torchvision import transforms, datasets Step 2. 딥러닝 모델 설계 시 필요한 장비 세팅123456if torch.cuda.is_available(): DEVICE = torch.device('cuda')else: DEVICE = torch.device('cpu')print(&quot;PyTorch Version:&quot;, torch.__version__, ' Device:', DEVICE) PyTorch Version: 1.9.0+cu102 Device: cuda 12BATCH_SIZE = 32 # 데이터가 32개로 구성되어 있음. EPOCHS = 10 # 전체 데이터 셋을 10번 반복해 학습함. Step 3. 데이터 다운로드 torchvision 내 datasets 함수 이용하여 데이터셋 다운로드 합니다. ToTensor() 활용하여 데이터셋을 tensor 형태로 변환 한 픽셀은 0255 범위의 스칼라 값으로 구성, 이를 01 범위에서 정규화 과정 진행 DataLoader는 일종의 Batch Size 만큼 묶음으로 묶어준다는 의미 Batch_size는 Mini-batch 1개 단위를 구성하는 데이터의 개수 12345678910111213141516train_dataset = datasets.MNIST(root = &quot;../data/MNIST&quot;, train = True, download = True, transform = transforms.ToTensor())test_dataset = datasets.MNIST(root = &quot;../data/MNIST&quot;, train = False, transform = transforms.ToTensor())train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle=False) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value=''))) Extracting ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Failed to download (trying next): HTTP Error 503: Service Unavailable Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value=''))) Extracting ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Failed to download (trying next): HTTP Error 503: Service Unavailable Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value=''))) Extracting ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Failed to download (trying next): HTTP Error 503: Service Unavailable Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value=''))) Extracting ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw /usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.) return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s) step 4. 데이터 확인 및 시각화 데이터를 확인하고 시각화를 진행합니다. 32개의 이미지 데이터에 label 값이 각 1개씩 존재하기 때문에 32개의 값을 갖고 있음 1234for (X_train, y_train) in train_loader: print('X_train:', X_train.size(), 'type:', X_train.type()) print('y_train:', y_train.size(), 'type:', y_train.type()) break X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor y_train: torch.Size([32]) type: torch.LongTensor 1234567pltsize = 1plt.figure(figsize=(10 * pltsize, pltsize))for i in range(10): plt.subplot(1, 10, i + 1) plt.axis('off') plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap=&quot;gray_r&quot;) plt.title('Class: ' + str(y_train[i].item())) step 5. MLP 모델 설계 torch 모듈을 이용해 MLP를 설계합니다. 12345678910111213141516171819class Net(nn.Module): ''' Forward Propagation 정의 ''' def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(28 * 28 * 1, 512) # (가로 픽셀 * 세로 픽셀 * 채널 수) 크기의 노드 수 설정 Fully Connected Layer 노드 수 512개 설정 self.fc2 = nn.Linear(512, 256) # Input으로 사용할 노드 수는 512으로, Output 노드수는 256개로 지정 self.fc3 = nn.Linear(256, 10) # Input 노드수는 256, Output 노드수는 10개로 지정 def forward(self, x): x = x.view(-1, 28 * 28) # 1차원으로 펼친 이미지 데이터 통과 x = self.fc1(x) x = F.sigmoid(x) x = self.fc2(x) x = F.sigmoid(x) x = self.fc3(x) x = F.log_softmax(x, dim = 1) return x step 6. 옵티마이저 목적 함수 설정 Back Propagation 설정 위한 목적 함수 설정 1234model = Net().to(DEVICE)optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.5)criterion = nn.CrossEntropyLoss() # output 값과 원-핫 인코딩 값과의 Loss print(model) Net( (fc1): Linear(in_features=784, out_features=512, bias=True) (fc2): Linear(in_features=512, out_features=256, bias=True) (fc3): Linear(in_features=256, out_features=10, bias=True) ) step 7. MLP 모델 학습 MLP 모델을 학습 상태로 지정하는 코드를 구현 1234567891011121314def train(model, train_loader, optimizer, log_interval): model.train() for batch_idx, (image, label) in enumerate(train_loader): # 모형 학습 image = image.to(DEVICE) label = label.to(DEVICE) optimizer.zero_grad() # Optimizer의 Gradient 초기화 output = model(image) loss = criterion(output, label) loss.backward() # back propagation 계산 optimizer.step() if batch_idx % log_interval == 0: print(&quot;Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loass: {:.6f}&quot;.format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) step 8. 검증 데이터 확인 함수1234567891011121314151617def evaluate(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for image, label in test_loader: image = image.to(DEVICE) label = label.to(DEVICE) output = model(image) test_loss += criterion(output, label).item() prediction = output.max(1, keepdim = True)[1] correct += prediction.eq(label.view_as(prediction)).sum().item() test_loss /= len(test_loader.dataset) test_accuracy = 100. * correct / len(test_loader.dataset) return test_loss, test_accuracy 모델 평가 시, Gradient를 통해 파라미터 값이 업데이트되는 현상 방지 위해 torch.no_grad() Gradient의 흐름 제어 step 9. MLP 학습 실행1234for Epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, log_interval=200) test_loss, test_accuracy = evaluate(model, test_loader) print(&quot;\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n&quot;.format(Epoch, test_loss, test_accuracy)) /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead. warnings.warn(&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;) Train Epoch: 1 [0/60000(0%)] Train Loass: 2.343919 Train Epoch: 1 [6400/60000(11%)] Train Loass: 2.304868 Train Epoch: 1 [12800/60000(21%)] Train Loass: 2.276658 Train Epoch: 1 [19200/60000(32%)] Train Loass: 2.320047 Train Epoch: 1 [25600/60000(43%)] Train Loass: 2.309569 Train Epoch: 1 [32000/60000(53%)] Train Loass: 2.293312 Train Epoch: 1 [38400/60000(64%)] Train Loass: 2.264061 Train Epoch: 1 [44800/60000(75%)] Train Loass: 2.311200 Train Epoch: 1 [51200/60000(85%)] Train Loass: 2.261489 Train Epoch: 1 [57600/60000(96%)] Train Loass: 2.260837 [EPOCH: 1], Test Loss: 0.0698, Test Accuracy: 36.43 % Train Epoch: 2 [0/60000(0%)] Train Loass: 2.233048 Train Epoch: 2 [6400/60000(11%)] Train Loass: 2.196863 Train Epoch: 2 [12800/60000(21%)] Train Loass: 2.173305 Train Epoch: 2 [19200/60000(32%)] Train Loass: 2.120425 Train Epoch: 2 [25600/60000(43%)] Train Loass: 2.023674 Train Epoch: 2 [32000/60000(53%)] Train Loass: 1.936951 Train Epoch: 2 [38400/60000(64%)] Train Loass: 1.738194 Train Epoch: 2 [44800/60000(75%)] Train Loass: 1.493235 Train Epoch: 2 [51200/60000(85%)] Train Loass: 1.547187 Train Epoch: 2 [57600/60000(96%)] Train Loass: 1.400939 [EPOCH: 2], Test Loss: 0.0402, Test Accuracy: 60.40 % Train Epoch: 3 [0/60000(0%)] Train Loass: 1.429194 Train Epoch: 3 [6400/60000(11%)] Train Loass: 1.178954 Train Epoch: 3 [12800/60000(21%)] Train Loass: 0.970049 Train Epoch: 3 [19200/60000(32%)] Train Loass: 1.105888 Train Epoch: 3 [25600/60000(43%)] Train Loass: 0.926736 Train Epoch: 3 [32000/60000(53%)] Train Loass: 0.794726 Train Epoch: 3 [38400/60000(64%)] Train Loass: 0.828843 Train Epoch: 3 [44800/60000(75%)] Train Loass: 0.872613 Train Epoch: 3 [51200/60000(85%)] Train Loass: 0.713790 Train Epoch: 3 [57600/60000(96%)] Train Loass: 0.464628 [EPOCH: 3], Test Loss: 0.0243, Test Accuracy: 76.67 % Train Epoch: 4 [0/60000(0%)] Train Loass: 0.507696 Train Epoch: 4 [6400/60000(11%)] Train Loass: 0.635314 Train Epoch: 4 [12800/60000(21%)] Train Loass: 0.636553 Train Epoch: 4 [19200/60000(32%)] Train Loass: 0.875113 Train Epoch: 4 [25600/60000(43%)] Train Loass: 0.558601 Train Epoch: 4 [32000/60000(53%)] Train Loass: 0.462972 Train Epoch: 4 [38400/60000(64%)] Train Loass: 0.630431 Train Epoch: 4 [44800/60000(75%)] Train Loass: 0.428842 Train Epoch: 4 [51200/60000(85%)] Train Loass: 0.826000 Train Epoch: 4 [57600/60000(96%)] Train Loass: 0.508183 [EPOCH: 4], Test Loss: 0.0182, Test Accuracy: 83.32 % Train Epoch: 5 [0/60000(0%)] Train Loass: 0.576632 Train Epoch: 5 [6400/60000(11%)] Train Loass: 0.437238 Train Epoch: 5 [12800/60000(21%)] Train Loass: 0.942570 Train Epoch: 5 [19200/60000(32%)] Train Loass: 0.412996 Train Epoch: 5 [25600/60000(43%)] Train Loass: 0.362784 Train Epoch: 5 [32000/60000(53%)] Train Loass: 0.677201 Train Epoch: 5 [38400/60000(64%)] Train Loass: 0.706629 Train Epoch: 5 [44800/60000(75%)] Train Loass: 0.587188 Train Epoch: 5 [51200/60000(85%)] Train Loass: 0.584579 Train Epoch: 5 [57600/60000(96%)] Train Loass: 0.602702 [EPOCH: 5], Test Loss: 0.0147, Test Accuracy: 86.63 % Train Epoch: 6 [0/60000(0%)] Train Loass: 0.388631 Train Epoch: 6 [6400/60000(11%)] Train Loass: 0.571896 Train Epoch: 6 [12800/60000(21%)] Train Loass: 0.287511 Train Epoch: 6 [19200/60000(32%)] Train Loass: 0.394190 Train Epoch: 6 [25600/60000(43%)] Train Loass: 0.264939 Train Epoch: 6 [32000/60000(53%)] Train Loass: 0.495090 Train Epoch: 6 [38400/60000(64%)] Train Loass: 0.274051 Train Epoch: 6 [44800/60000(75%)] Train Loass: 0.299830 Train Epoch: 6 [51200/60000(85%)] Train Loass: 0.450571 Train Epoch: 6 [57600/60000(96%)] Train Loass: 0.257513 [EPOCH: 6], Test Loss: 0.0129, Test Accuracy: 87.99 % Train Epoch: 7 [0/60000(0%)] Train Loass: 0.377794 Train Epoch: 7 [6400/60000(11%)] Train Loass: 0.630070 Train Epoch: 7 [12800/60000(21%)] Train Loass: 0.320578 Train Epoch: 7 [19200/60000(32%)] Train Loass: 0.658281 Train Epoch: 7 [25600/60000(43%)] Train Loass: 0.643368 Train Epoch: 7 [32000/60000(53%)] Train Loass: 0.420115 Train Epoch: 7 [38400/60000(64%)] Train Loass: 0.687097 Train Epoch: 7 [44800/60000(75%)] Train Loass: 0.430246 Train Epoch: 7 [51200/60000(85%)] Train Loass: 0.343727 Train Epoch: 7 [57600/60000(96%)] Train Loass: 0.577327 [EPOCH: 7], Test Loss: 0.0120, Test Accuracy: 88.78 % Train Epoch: 8 [0/60000(0%)] Train Loass: 0.530810 Train Epoch: 8 [6400/60000(11%)] Train Loass: 0.278406 Train Epoch: 8 [12800/60000(21%)] Train Loass: 0.140812 Train Epoch: 8 [19200/60000(32%)] Train Loass: 0.471123 Train Epoch: 8 [25600/60000(43%)] Train Loass: 0.415495 Train Epoch: 8 [32000/60000(53%)] Train Loass: 0.534980 Train Epoch: 8 [38400/60000(64%)] Train Loass: 0.318894 Train Epoch: 8 [44800/60000(75%)] Train Loass: 0.444783 Train Epoch: 8 [51200/60000(85%)] Train Loass: 0.211995 Train Epoch: 8 [57600/60000(96%)] Train Loass: 0.358874 [EPOCH: 8], Test Loss: 0.0113, Test Accuracy: 89.73 % Train Epoch: 9 [0/60000(0%)] Train Loass: 0.423838 Train Epoch: 9 [6400/60000(11%)] Train Loass: 0.225967 Train Epoch: 9 [12800/60000(21%)] Train Loass: 0.348597 Train Epoch: 9 [19200/60000(32%)] Train Loass: 0.397753 Train Epoch: 9 [25600/60000(43%)] Train Loass: 0.199611 Train Epoch: 9 [32000/60000(53%)] Train Loass: 0.269096 Train Epoch: 9 [38400/60000(64%)] Train Loass: 0.311267 Train Epoch: 9 [44800/60000(75%)] Train Loass: 0.575633 Train Epoch: 9 [51200/60000(85%)] Train Loass: 0.246814 Train Epoch: 9 [57600/60000(96%)] Train Loass: 0.272062 [EPOCH: 9], Test Loss: 0.0108, Test Accuracy: 89.94 % Train Epoch: 10 [0/60000(0%)] Train Loass: 0.399807 Train Epoch: 10 [6400/60000(11%)] Train Loass: 0.282056 Train Epoch: 10 [12800/60000(21%)] Train Loass: 0.270553 Train Epoch: 10 [19200/60000(32%)] Train Loass: 0.605585 Train Epoch: 10 [25600/60000(43%)] Train Loass: 0.333442 Train Epoch: 10 [32000/60000(53%)] Train Loass: 0.395121 Train Epoch: 10 [38400/60000(64%)] Train Loass: 0.676208 Train Epoch: 10 [44800/60000(75%)] Train Loass: 0.170694 Train Epoch: 10 [51200/60000(85%)] Train Loass: 0.160667 Train Epoch: 10 [57600/60000(96%)] Train Loass: 0.381771 [EPOCH: 10], Test Loss: 0.0105, Test Accuracy: 90.28 % train 함수 실행하면, model은 기존에 정의한 MLP 모델, train_loader는 학습 데이터, optimizer는 SGD, log_interval은 학습이 진행되면서 mini-batch index를 이용해 과정을 모니터링할 수 있도록 출력함. 학습 완료 시, Test Accuracy는 90% 수준의 정확도를 나타냄.","link":"/2021/10/21/step02_2_MLP/"}],"tags":[],"categories":[]}